Roban Hultman Kramer

Hightable

All of these dichotomies are false.

Observational Astrophysics versus Applied Machine Learning

 "uncertainty is everything" vs. "decisions are everything"

examples

  Bayesian MCMC versus SVM: focus effort on exploring parameter space
  (space occupied by the weight vector) versus finding a decision
  boundary in feature space (the space of the data)

or

 a focus on constraining parameters and model checking versus
 predictions and accuracy

counter-example: observation planning and target selection

 http://arxiv.org/abs/1101.4965

 instead of showing a limited number of recommendations to a user, or
 spending a limited marketing budget on targeted ads

 spend a limited amount of telescope time observing potentially
 interesting objects (HST oversubscribed by a factor of 6 according
 to the note I got rejecting my last proposal)

or 

 Error bars, p-values, and posterior distributions versus F_n scores,
 lift, and ROC curves

 Of course, in astronomy we like to plot 1-sigma error bars in log
 space and are comfortable with them spanning orders of magnitude. A
 factor of two is easily swept under the rug with the phrase
 "excluding systematic error".

 figure 4: http://arxiv.org/abs/0912.4263

Machine Learning versus Statistics

 The big difference I’ve noticed between the two fields is that
 statisticians like to demonstrate our methods on new examples whereas
 computer scientists seem to be prefer to show better performance on
 benchmark problems. Both approaches to evaluation make sense in their
 own way; I just have the impression that stat and CS have evolved to
 have different priorities. To a statistician, a method is powerful
 when it generalizes to new situations. To a computer scientist,
 though, solving a new problem is no big deal—they can solve problems
 whenever they want, and it is through benchmarks that they can make
 fair comparisons.

 Economics: identify the causal effect;

 Psychology: model the underlying process;

 Statistics: fit the data;

 Computer science: predict.

 -- http://andrewgelman.com/2012/09/model-checking-and-model-understanding-in-machine-learning/

Models with high computational complexity versus models with high data
size (dimensions and samples).

 figure 6 from http://ba.stat.cmu.edu/journal/2012/vol07/issue03/solonen.pdf

vs.

 mahout

Counter-example: SKA

Quasar Jet Image (http://hubblesite.org/newscenter/archive/releases/2000/20/image/a/) / Quasar spectrum

 jet extends 1,500 parsecs / 5,000 light-years 
 Credit: NASA and The Hubble Heritage Team (STScI/AURA)

Quasar fit parameters

Hierarchical model / IGM fit parameters
